<!DOCTYPE html><html><head><base href="/"><meta charset="utf8"><meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1"><meta property="fb:admins" content="1482073503"><meta name="viewport" content="width=device-width, initial-scale=1"><meta property="og:title" content="Vorverarbeitung von Text für Machine Learning in node.js"><meta name="description" content="Für die Klassifzierung von Texten auf Wortebene, ist die Vorverarbeitung eine wichtige Grundlage. Hierfür haben wir einen OpenNLP Wrapper für node.js verwendet. Ein solider Wrapper, jedoch tokenisiert dieser nur sequentiell, sodass wir einen neuen OpenNLP Wrapper geschrieben haben, der parallel arbeitet."><meta name="author" content="Alex Klein"><meta property="og:type" content="article"><meta property="og:url" content="http://rocketloop.de/posts/textvorverarbeitung-machine-learning.html"><meta property="og:image" content="http://rocketloop.de/img/logo_square_big.png"><meta property="og:description" content="Für die Klassifzierung von Texten auf Wortebene, ist die Vorverarbeitung eine wichtige Grundlage. Hierfür haben wir einen OpenNLP Wrapper für node.js verwendet. Ein solider Wrapper, jedoch tokenisiert dieser nur sequentiell, sodass wir einen neuen OpenNLP Wrapper geschrieben haben, der parallel arbeitet."><title>Vorverarbeitung von Text für Machine Learning in node.js</title><link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,400italic" rel="stylesheet" type="text/css"><link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,500" rel="stylesheet" type="text/css"><script src="https://code.jquery.com/jquery-2.2.0.min.js"></script><link rel='stylesheet' href='css/master.css' /></head><body itemscope itemtype="http://schema.org/LocalBusiness" class="typography"><link itemprop="url" href="http://rocketloop.de"><header class="header"><a href="/" class="header__logo-link"><img src="/img/logo-inverted.png" alt="Rocketloop" class="header__logo"></a><ul class="header__navigation navigation"><li class="navigation__item"><a href="/" class="navigation__item-link">Home</a></li><li class="navigation__item"><a href="/posts" class="navigation__item-link navigation__item-link--active">Blog</a></li></ul><a id="contactButton" href="#contact" class="header__call-to-action button button--inverted">Kontaktieren</a></header><section class="page-section blog-header-section"><h1 class="blog-header-section__title typography--headline0"><a href="/posts" class="blog-header-section__title-link">Der Rocketloop Blog</a></h1><div class="blog-header-section__stars-layer blog-header-section__stars-layer--background"></div><div class="blog-header-section__stars-layer blog-header-section__stars-layer--foreground"></div></section><section class="page-section blog-section"><div class="page-section__wrap"><article class="blog-post"><div class="blog-post__meta">Dienstag, 12. April 2016</div><h1 class="blog-post__title typography--headline1">Vorverarbeitung von Text für Machine Learning in node.js</h1><section class="blog-post__content"><p>Für die Verwendung großer Daten für Machine Learning Verfahren bedarf es erst einer sorgfältigen Vorverarbeitung, die grundlegend ist für die Genauigkeit der Ausgabe.
Betrachten wir die Klassifizierung von Texten auf Wortebene, so ist es wichtig, dass jegliche Texte vorverarbeitet werden, sprich jeder Text wird in Sätze unterteilt und dieser danach in einzelne Wörter.
Der OpenNLP Wrapper für Node.js ist optimal dafür geeignet diese Vorverarbeitung zu leisten, denn dieser ist nicht nur detailliert  konfigurierbar, sondern unterliegt auch der MIT License, sodass der private und kommerzielle Gebrauch kostenlos ist.</p>
<h2 id="installation">Installation</h2>
<pre class="highlight"><code>npm install opennlp
</code></pre><p>OpenNLP bietet folgende Verfahren einen Text zu verarbeiten/analysieren:</p>
<ul>
<li>Chunker</li>
<li>Name Finder</li>
<li>Part of Speach Tagging</li>
<li>Sentence Detector</li>
<li>Tokenizer</li>
</ul>
<p>Dafür stellt Apache unter <a href="http://opennlp.sourceforge.net/models-1.5/">http://opennlp.sourceforge.net/models-1.5/</a> eine Vielzahl trainierter Modelle zur Verfügung, die in verschiedenen Sprachen (zum Beispiel Englisch und Deutsch) heruntergeladen und eingebunden werden können.
Zusätzlich benötigt man die opennlp-tools-1.6.0.jar, die <a href="https://opennlp.apache.org/announcement/release-160.html">hier</a> heruntergeladen werden kann. </p>
<h2 id="konfiguration">Konfiguration</h2>
<p>In der folgenden Konfiguration wird der OpenNLP Wrapper mit den deutschen Modellen für den Sentence Detector und den Tokenizer initialisiert. </p>
<pre class="highlight"><code><span class="kd">var</span> <span class="nx">openNLPOptions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;models&quot;</span> <span class="o">:</span> <span class="p">{</span>
        <span class="s2">&quot;tokenizer&quot;</span><span class="o">:</span> <span class="nx">nlpPath</span> <span class="o">+</span> <span class="s1">&#39;/de-token.bin&#39;</span><span class="p">,</span>
        <span class="s2">&quot;sentenceDetector&quot;</span><span class="o">:</span> <span class="nx">nlpPath</span> <span class="o">+</span> <span class="s1">&#39;/de-sent.bin&#39;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="s2">&quot;jarPath&quot;</span><span class="o">:</span> <span class="nx">nlpPath</span> <span class="o">+</span> <span class="s2">&quot;/opennlp-tools-1.6.0.jar&quot;</span>
<span class="p">};</span>
</code></pre><p>Der große Vorteil hierbei ist, dass für die Verarbeitung einer anderen Sprache lediglich die Modelle ausgetauscht und sonst keinerlei Änderungen vorgenommen werden müssen.</p>
<h2 id="sentence-detector">Sentence Detector</h2>
<p>Der Sentence Detector splittet die Texteingabe bei jedem Punkt und erkennt, ob es sich dabei um ein Satzende handelt oder um einen Punkt in einer Datumsangabe oder Abkürzung.</p>
<pre class="highlight"><code><span class="kd">var</span> <span class="nx">openNLP</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s2">&quot;opennlp&quot;</span><span class="p">);</span>
<span class="kd">var</span> <span class="nx">sentence</span> <span class="o">=</span> <span class="s1">&#39;Am 13. Juni 2014 wurde die deutsche Fußball Nationalmannschaft &#39;</span> <span class="o">+</span> 
               <span class="s1">&#39;Weltmeister. Das war der 4. Weltmeister Titel!&#39;</span><span class="p">;</span>
<span class="kd">var</span> <span class="nx">sentenceDetector</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">openNLP</span><span class="p">().</span><span class="nx">sentenceDetector</span><span class="p">;</span>
<span class="nx">sentenceDetector</span><span class="p">.</span><span class="nx">sentDetect</span><span class="p">(</span><span class="nx">sentence</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">results</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">results</span><span class="p">)</span>
    <span class="cm">/*</span>
<span class="cm">    Ausgabe: </span>
<span class="cm">    [ </span>
<span class="cm">        &#39;Am 13. Juni 2014 wurde die deutsche Fußball  Nationalmannschaft Weltmeister.&#39;, </span>
<span class="cm">        &#39;Das war der 4. Weltmeister Titel!&#39;</span>
<span class="cm">    ]</span>
<span class="cm">    */</span>
<span class="p">});</span>
</code></pre><h2 id="tokenizer">Tokenizer</h2>
<p>Der Tokenizer splittet den Satz in einzelne Wörter und Satzzeichen auf. Leerzeichen zwischen Satzzeichen werden nicht verworfen, sondern als Token erkannt.</p>
<pre class="highlight"><code><span class="kd">var</span> <span class="nx">openNLP</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s2">&quot;opennlp&quot;</span><span class="p">);</span>
<span class="kd">var</span> <span class="nx">sentences</span> <span class="o">=</span> <span class="s2">&quot;Am 12. Juni 2014 wurde die deutsche Fußball Nationalmannschaft Weltmeister.&quot;</span><span class="p">;</span>
<span class="kd">var</span> <span class="nx">tokenizer</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">openNLP</span><span class="p">().</span><span class="nx">tokenizer</span><span class="p">;</span>
<span class="nx">tokenizer</span><span class="p">.</span><span class="nx">tokenize</span><span class="p">(</span><span class="nx">sentence</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">results</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">results</span><span class="p">);</span>
    <span class="cm">/*</span>
<span class="cm">    Ausgabe: [ </span>
<span class="cm">                &#39;Am&#39;,</span>
<span class="cm">                &#39;12&#39;,</span>
<span class="cm">                &#39;.&#39;,</span>
<span class="cm">                &#39;Juni&#39;,</span>
<span class="cm">                &#39;2014&#39;,</span>
<span class="cm">                &#39;wurde&#39;,</span>
<span class="cm">                &#39;die&#39;,</span>
<span class="cm">                &#39;deutsche&#39;,</span>
<span class="cm">                &#39;Fußball&#39;,</span>
<span class="cm">                &#39;Nationalmannschaft&#39;,</span>
<span class="cm">                &#39;Weltmeister&#39;,</span>
<span class="cm">                &#39;.&#39; </span>
<span class="cm">            ]</span>
<span class="cm">    */</span>
<span class="p">})</span>
</code></pre><h2 id="optimierung">Optimierung</h2>
<p>Verwendet man eine große Datenmenge an Text, so ist der OpenNLP Wrapper sehr gut, um die Texte zu verarbeiten, jedoch arbeitet dieser nicht effizient, da die Tokenisierung sequentiell abläuft und nicht parallel. Da die Dauer der Tokenisierung somit linear mit der Satzanzahl ansteigt, mussten wir hierfür eine effiziente Lösung finden.
Dazu haben wir uns einen eigenen minimalistischen OpenNLP Wrapper geschrieben, der den Sentence Detector und den Tokenizer beinhaltet. Dieser ist auf <a href="https://github.com/flore2003/opennlp-wrapper">https://github.com/flore2003/opennlp-wrapper</a> zu finden und unterliegt der MIT License. Der Wrapper ist somit nicht vollkompatibel zum zuvor beschriebenen Wrapper.</p>
<h3 id="installation">Installation</h3>
<pre class="highlight"><code>npm install opennlp-wrapper
</code></pre><p>Die Konfiguration, Sentence Splitting und Tokenisierung unterscheidet sich hierbei leicht vom originalen Projekt, sodass ein einfacher Tausch der Bibliothek nicht genügt.</p>
<h3 id="konfiguration">Konfiguration</h3>
<p>Die folgende Konfiguration beschreibt den OpenNLP Wrapper, der von uns entwickelt wurde, mit den deutschen Modellen für den Sentence Detector und den Tokenizer initialisiert. Man beachte, dass auch hier die Konfiguration leicht angepasst wurde, im Vergleich zur original Bibliothek.</p>
<pre class="highlight"><code><span class="kd">var</span> <span class="nx">openNLPOptions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="nx">models</span> <span class="o">:</span> <span class="p">{</span>
        <span class="s2">&quot;tokenizer&quot;</span><span class="o">:</span> <span class="nx">nlpPath</span> <span class="o">+</span>  <span class="s1">&#39;de-token.bin&#39;</span><span class="p">,</span>
        <span class="s2">&quot;sentenceDetector&quot;</span><span class="o">:</span> <span class="nx">nlpPath</span> <span class="o">+</span> <span class="s1">&#39;de-sent.bin&#39;</span>
    <span class="p">},</span>
    <span class="s2">&quot;jarPath&quot;</span><span class="o">:</span> <span class="nx">nlpPath</span> <span class="o">+</span>  <span class="s2">&quot;opennlp-tools-1.6.0.jar&quot;</span>
<span class="p">};</span>
<span class="kd">var</span> <span class="nx">openNLP</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">OpenNLP</span><span class="p">(</span><span class="nx">openNLPOptions</span><span class="p">);</span>
</code></pre><h3 id="sentence-detector">Sentence Detector</h3>
<p>In der originalen Bibliothek wurde auf der OpenNLP Instanz sentenceDetector.sentDetect() aufgerufen. Hier genügt lediglich der Methodenaufruf <strong>detectSentences()</strong> auf der OpenNLP Instanz.</p>
<pre class="highlight"><code><span class="kd">var</span> <span class="nx">input</span> <span class="o">=</span> <span class="s1">&#39;Am 12. Juni 2014 wurde die Fußball Nationalmannschaft &#39;</span> <span class="o">+</span> 
            <span class="s1">&#39;Fußballweltmeister. Das war der 4. Weltmeister Titel!&#39;</span><span class="p">;</span>

<span class="nx">openNLP</span><span class="p">.</span><span class="nx">detectSentences</span><span class="p">(</span><span class="nx">input</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">sentences</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">sentences</span><span class="p">);</span>
<span class="p">});</span>
</code></pre><h3 id="tokenizer">Tokenizer</h3>
<p>In der originalen Bibliothek wurde auf der OpenNLP Instanz  tokenizer.tokenize aufgerufen. Hier genügt lediglich der Methodenaufruf <strong>tokenize()</strong> auf der OpenNLP Instanz.</p>
<pre class="highlight"><code><span class="kd">var</span> <span class="nx">input</span> <span class="o">=</span> <span class="s1">&#39;Am 12. Juni 2014 wurde die Fußball Nationalmannschaft Fußballweltmeister.&#39;</span><span class="p">;</span>

<span class="nx">openNLP</span><span class="p">.</span><span class="nx">tokenize</span><span class="p">(</span><span class="nx">input</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">tokens</span><span class="p">)</span> <span class="p">{</span>
    <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">tokens</span><span class="p">);</span>
<span class="p">});</span>
</code></pre><h2 id="anmerkungen-und-tipps">Anmerkungen und Tipps</h2>
<p>Damit die Daten für ein Machine Learning Verfahren, wie zum Beispiel Naive Bayes, geeignet sind, bedarf es noch weiteren Vorverarbeitungsschritten, wie die Verwendung eines Stemmers oder Lemmatisierers.
Für die Verarbeitung von deutschem Text haben wir einen Stemmer verwendet, der folgendes Problem löst:
Man betrachte die Wörter &quot;Kauf, Käufe, kaufen, kaufe&quot;, die sich alle auf den Erwerb eines Artikels beziehen, jedoch unterschiedlich verwendet werden, aber dennoch das Gleiche aussagen. Der Stemmer extrahiert aus jedem Wort den Wortstamm, was in diesem Fall &quot;kauf&quot; entspricht. Führt man dieses Verfahren auf allen Wörtern durch, die man mittels des Tokenisierens ermittelt hat, so schrumpft die Anzahl unterschiedlicher Wörter um ein Vielfaches.</p>
<p>Der <a href="https://github.com/hthetiot/node-snowball">Snowball Stemmer</a> ist hierfür sehr zu empfehlen, da er nicht nur sehr einfach zu verwenden ist, sondern auch über 13 verschiedene Sprachen unterstützt.</p>
<p>Nachdem man alle Wörter gestemmt hat sollte man noch die Stopworte entfernen. Hierzu zählen im Deutschen unter anderem bestimmte Artikel, unbestimmte Artikel und Konjunktionen. Im Englischen sind die Wörter &#39;a&#39;, &#39;of&#39;, &#39;the&#39;, &#39;I&#39;, &#39;it&#39;, &#39;you&#39; und &#39;and&#39;  Teil der Stopwörter.
Eine sehr gute Variante Stopworte zu bestimmten, ist nach dem Stemming alle Wörter nach absteigender Häufigkeit zu sortieren und die ersten 100 bis 200 Wörter dieser Liste zu betrachten und zu analysieren, welche Wörter wirklich nicht aussagekräftig sind und verworfen werden können.</p>
<p>Im nächsten Beitrag bauen wir auf diesem Wissen auf und entwickeln einen Naive Bayes Klassifizierer, der beispielsweise in der Spamerkennung eingesetzt wird.</p>
</section><div class="blog-post__author"><span>Geschrieben von </span><span>Alex Klein</span></div><h2 class="blog-post__comments-headline typography--headline2">Kommentare</h2><div id="disqus_thread"></div><script>var disqus_config = function () {
    // this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = "textvorverarbeitung-machine-learning"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//rocketloop.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript></article></div></section><section id="contact" class="page-section contact-section"><div class="page-section__wrap"><h2 class="contact-section__heading typography--headline2">Wir sind bereit! Was ist mit Ihnen?</h2><p class="contact-section__subheading typography--headline2"> Schreiben Sie uns einfach an  <a href="mailto:hello@rocketloop.de" class="contact-section__heading-link">hello@rocketloop.de</a></p><div class="contact-section__contact-info contact-info"><h3 class="contact-info__heading typography--headline4">Kontakt</h3><h4 itemprop="alternateName" class="contact-info__contact-name typography--subheadline">Rocketloop</h4><h4 itemprop="name" class="contact-info__contact-name typography--subheadline">Helmig Klein Reifschneider GbR</h4><div itemprop="address" itemscope itemtype="http://schema.org/PostalAddress" class="contact-info__contact-address"><div class="contact-info__contact-street"><span itemprop="streetAddress">Trakehner Str. 7-9, Eingang B  </span></div><div class="contact-info__contact-city"><span itemprop="postalCode">60487  </span><span itemprop="addressLocality">Frankfurt am Main</span></div><div itemprop="addressCountry" class="contact-info__contact-country">Deutschland</div></div><div class="contact-info__contact-phone"> tel.  <a itemprop="telephone" href="tel:+496934876289" class="contact-info__contact-link">069 / 34876289</a></div><div class="contact-info__contact-mail"> mail.  <a href="mailto:hello@rocketloop.de" itemprop="email" class="contact-info__contact-link">hello@rocketloop.de</a></div><div class="contact-info__contact-founders"><h5 class="typography--subheadline">Gesellschafter</h5><p><span itemprop="founder" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Patrick Helmig</span></span>, <span itemprop="founder" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Alex Klein  </span></span> und  <span itemprop="founder" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Florian Reifschneider </span></span></p></div><div class="contact-info__contact-identification"><h5 class="typography--subheadline">Umsatzsteuer-Identifikationsnummer gem. § 27a UStG</h5><p itemprop="taxID">DE 310970201</p></div>
<p class="contact-section__footer"> Made with  <span class="contact-section__footer-highlight">♥  </span> by Rocketloop</p></div></div></section><script src='js/build.js'></script></body></html>